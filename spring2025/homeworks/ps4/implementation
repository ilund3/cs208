import numpy as np
import matplotlib.pyplot as plt

def dp_beta(dataset, b, eps):
    # Clip data in [-b, b] interval
    n_rows = len(dataset)
    clip = [[min(max(val, -b), b) for val in row] for row in dataset]

    # Calculate and store the sufficient stats
    S_xy = 0.0
    S_xx = 0.0
    for i in range(n_rows):
        x_val = clip[i][0]
        y_val = clip[i][1]
        S_xy += x_val * y_val
        S_xx += x_val * x_val
    
    # Sensitivity for S_xy is 2*b^2, for S_xx is b^2, and split privacy budget equally.
    xy_scale = 4 * b**2 / eps
    xx_scale = 2 * b**2 / eps
    
    S_xy_noise = S_xy + np.random.laplace(loc=0, scale=xy_scale)
    S_xx_noise = S_xx + np.random.laplace(loc=0, scale=xx_scale)
    
    return S_xy_noise / S_xx_noise

# Perform the regression by aggregating the data and storing points for the ratio between the sufficients stats
def ols_regression(dataset):
    total_xy = 0.0
    total_xx = 0.0
    n_rows = len(dataset)
    for i in range(n_rows):
        x_val = dataset[i][0]
        y_val = dataset[i][1]
        total_xy += x_val * y_val
        total_xx += x_val * x_val
    if total_xx == 0:
        return 0.0
    return total_xy / total_xx

# Monte Carlo
trials = 1000  # number of trials per sample ize
sample_sizes = list(range(100, 5001, 100))  # sample sizes

ols_bias_vals = []
ols_std_vals = []
dp_bias_vals = []
dp_std_vals = []

for n in sample_sizes:
    ols_results = []
    dp_results = []
    # Run trials for each sample size
    for t in range(trials):
        # Generate the synthetic dataset
        X = []
        Y = []
        for i in range(n):
            # Draw uniformly from [-0.5, 0.5]
            x_val = np.random.uniform(-0.5, 0.5)
            # Generate y with added noise
            noise_val = np.random.normal(0, np.sqrt(0.02))
            y_val = x_val + noise_val
            X.append(x_val)
            Y.append(y_val)
        # Build dataset
        current_data = []
        for i in range(n):
            current_data.append([X[i], Y[i]])
        
        # Find OLS and DP estimates using functions from above
        est_ols = ols_regression(current_data)
        est_dp = dp_beta(current_data, 1.0, 0.1)
        
        ols_results.append(est_ols)
        dp_results.append(est_dp)
    
    # Find OLS estimates mean
    mean_ols = 0.0
    for val in ols_results:
        mean_ols += val
    mean_ols /= len(ols_results)
    
    mean_dp = 0.0
    for val in dp_results:
        mean_dp += val
    mean_dp /= len(dp_results)
    
    # Find bias (substract actual from the esimated)
    ols_bias_vals.append(mean_ols - 1.0)
    dp_bias_vals.append(mean_dp - 1.0)
    
    # Find std from OLS
    var_ols = 0.0
    for val in ols_results:
        var_ols += (val - mean_ols)**2
    var_ols /= len(ols_results)
    std_ols = var_ols**0.5
    
    var_dp = 0.0
    for val in dp_results:
        var_dp += (val - mean_dp)**2
    var_dp /= len(dp_results)
    std_dp = var_dp**0.5
    
    ols_std_vals.append(std_ols)
    dp_std_vals.append(std_dp)

# Plot results
plt.figure(figsize=(12, 6))

# Bias (Y) vs sample size (X)
plt.subplot(1, 2, 1)
plt.plot(sample_sizes, ols_bias_vals,  color='blue', label='OLS Bias', marker='o')
plt.plot(sample_sizes, dp_bias_vals, color='red', label='DP Bias', marker='o')
plt.axhline(0, color='black', linestyle='--')
plt.ylim(-1.0, 1.0)
plt.xlabel('Sample Size (n)')
plt.ylabel('Bias (Mean Error)')
plt.title('Bias vs Sample Size')
plt.legend()

# Std Deviation (Y) vs sample size(X)
plt.subplot(1, 2, 2)
plt.plot(sample_sizes, ols_std_vals, color='green', marker='o', label='OLS Std')
plt.plot(sample_sizes, dp_std_vals, color='magenta', marker='o', label='DP Std')
plt.ylim(0, 1.0)
plt.xlabel('Sample Size (n)')
plt.ylabel('Standard Deviation')
plt.title('Standard Deviation vs Sample Size')
plt.legend()

plt.tight_layout()
plt.show()