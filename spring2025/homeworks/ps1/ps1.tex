\documentclass[11pt]{article}

\def\draft{0}

\input{hw_style}

\title{\vspace{-1.5cm} HW 1:  Probability Review and Reidentification Attacks}
\author{CS 2080 Applied Privacy for Data Science, Spring 2025}
\date{\textbf{Version 2}: Due Friday, Feb. 7, 5pm.}


\begin{document}
\maketitle

\vspace{-3ex}

\instructions

\begin{enumerate}[leftmargin=*]

\item \textbf{Probability Review}

\begin{enumerate}
    \item  Let $S\sim \Bin(n,p)$ be a binomial random variable.  That is, $S=X_1+X_2+\cdots+X_n$, where 
    $X_1,\ldots,X_n$ are independent $\{0,1\}$-valued Bernoulli random variables where $\Pr[X_i = 1]=p$ (i.e. coin tosses where the probability of heads is $p$).  Calculate the standard deviation $\sigma[S]$. \\
    
    \underline{Hint:} \textit{recall that if $X$ and $Y$ are independent random variables, then $\Var[X+Y]=\Var[X]+\Var[Y]$, where $\Var$ denotes the variance.} \\

    \item Let $Z_1,\ldots,Z_k$ be independent random variables that are drawn from a Gaussian distribution $\Normal(0, \sigma^2)$, let $M=\max\{|Z_1|,|Z_2|,\ldots,|Z_k|\}$ and let $\Phi : \R\rightarrow [0,1]$ be the CDF of a standard normal $\Normal(0,1)$ distribution.  Show that for every $t>0$
    $$\Pr[M \geq t\sigma ] = 1- (1 - 2\Phi(-t))^k$$

    \label{part:maxnormals-exact}

    \item Now show that for every $t > 0$, $$\Pr[M \geq t\sigma ] \leq 2k\cdot \Phi(-t)$$
    \label{part:ineq}
    
    \item It is known that for all $x\geq 0$, we have 
    $$\Phi(-x) \leq \frac{1}{\sqrt{2\pi}}\cdot \frac{1}{x}\cdot e^{-x^2/2}$$
    Using this fact and Parts~\ref{part:maxnormals-exact} and \ref{part:ineq}, show that for $t = \sqrt{2\ln k+7}$, we have
    $$\Pr[M \geq t\sigma] < .01,$$
    where $M$ is defined as in Part~\ref{part:maxnormals-exact}.

    
    \item Let $S_1,\ldots,S_k$ be independent $\Bin(n,p)$ random variables.  The Central Limit Theorem (CLT) implies that as $n\rightarrow \infty$, each $Y_i=(S_i-\Exp[S_i])/\sigma[S_i]$ converges in distribution to a standard $\Normal(0,1)$ normal distribution. Pretending that $Y_i$ is actually a normal distribution (i.e. ignoring the rate of convergence in the CLT\footnote{While we have ignored the rate of convergence in the Central Limit Theorem here, similar bounds with slightly worse constants can be proven rigorously using ``Chernoff-Hoeffding Bounds,'' provided that $p(1-p)n\geq c\log k$ for an appropriate constant $c$}), show that
    $$\Pr\left[\max_i |S_i-pn| \geq \sqrt{2\ln k + 7} \cdot\sqrt{p(1-p) n}\right] < .01$$
      
    
    \item Review the definitions of asymptotic notation in Section 1 notes or Section 3.1 of the Cormen-Leiserson-Rivest-Stein text. 

    Fill in the table below with T (true) or F (false) to indicate the relationship between $f$ and $g$. For example, if $f=O(g)$, the first cell of the row should be T.


    \begin{center}
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
         $f$ & $g$ & $O$ & $o$ & $\Omega$ & $\omega$ & $\Theta$\\
         \hline
         $n^2 + 3n + 7$ & $10n^3 + 5n$ & & & & & \\            
         \hline
         $\log ( n^{\sqrt{n}})$ & $4\sqrt{n \log n}$ & & & & & \\
         \hline
         $n + 2\log n$ & $n$ & & & & & \\
         \hline
         $3^n$ & $n^3 2^n$ & & & & & \\
         \hline
         $\log(n^3 + 1)$ & $(\log n) + 10$ & & & & & \\
         \hline
    \end{tabular}
\end{center}

    
    Above and throughout the course, $\log$ denotes the logarithm base 2, and $\ln$ denotes the logarithm base $e$.
    

\end{enumerate}

\item \textbf{Reidentification Attack}

In the GitHub repo,\footnote{\url{https://github.com/opendp/cs208/tree/main/spring2025/data}} you will find the Public Use Micro Sample (PUMS) dataset from the 2000 US Census \texttt{FultonPUMS5full.csv}.  This is a sample from the ``Long Form'' from Georgia residents, which contained many more questions than the regular questionnaire, and was randomly assigned to some individuals during the decennial Census. (It has since been replaced by a continuously collected survey known as the \emph{American Community Survey}.)  

Also in that folder is the codebook file for the PUMS dataset that lists the variables available in the release.  Note this is the 5\% sample which means that five percent of records are randomly sampled and released.
Assume that there was no disclosure avoidance techniques applied to this data.

In the style of Latanya Sweeney's record linkage reidentification attack,\footnote{\url{https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1748-720X.1997.tb01885.x}} in this problem you will propose a reidentification attack on the PUMS dataset by identifying demographic variables that, if known from another auxiliary source, could uniquely identify individuals.  Note that while Sweeney used zipcodes as the geographic indicator, individuals in this Census release are identified by Public Use Microdata Areas (PUMAs) which are Census constructed geographic areas that contain at least 100,000 individuals. 

\begin{enumerate}
    \item Create a new Jupyter notebook and read in the PUMS dataset. For instructions on setting up a programming environment, installing Jupyter, and running your first notebook, see the \href{https://github.com/opendp/cs208/blob/main/spring2025/sections/section0-programming.pdf}{section 0 notes}. It is also fine if you prefer to work on Google Colab or other python IDEs. 

    \item Determine the variables that you would match across the auxiliary source and the PUMS dataset. 
 
    \begin{enumerate}

        \item Write a function that takes in a dataset and a set of features/variables for that dataset, and returns the fraction of individuals in the dataset who are unique with respect to the specified variables. \footnote{Note there is also a short subset of the data in the file \texttt{FultonPUMS5sample100.csv} which might be useful for testing purposes as you write your function.}
        \item Using your function, and your proposed reidentification attack using an auxiliary source, what is the fraction of unique individuals in the dataset you could attempt to reidentify from your proposed attack?  \\

        \underline{Note on the auxiliary source:} You do not need to find a specific external dataset for the auxiliary source. You could simply explain what is the auxiliary knowledge that you need as an adversary to make the reidentification attack successful by:
        \begin{itemize}
            \item[--] Providing a list of three potential auxiliary sources.
            \item[--] Arguing how the auxiliary knowledge needed for your attack could be found in these sources, which could simply be suggesting that a certain set of variables and individuals are likely to be present in the auxiliary sources.\\
        \end{itemize}
        
        \item 
        Recall that this is a 5\% sample from the full Census data.   As a ``back-of-the-envelope" calculation, roughly approximate what fraction of individuals would you expect to be unique if you could instead run your function on the entire Census dataset? Write a few sentences stating the assumptions underlying your calculation.\footnote{Hint: There are many ways to go about this, either analytically with some simplifying assumptions, or numerically with a simulation.  Analytically, if an individual has a $p$ chance of being unique among $N$ individuals, then think about what assumption you'd make to be able to say they have roughly a $p^k$ chance of being unique among $kN$ individuals.  

        Numerically, you could instead plot the value your function from part (iii.) gives you as you use subsamples of the available data and increase the sample size up to the current size of the data, and then try to project that curve out to where it would be with 20 times that amount of data.}   Your logic is more important than the accuracy of the number itself.  

        
    \end{enumerate}
\end{enumerate}


\end{enumerate}
\iffalse
\begin{thebibliography}

\bibitem{CormenLeRiSt09} Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford, ``Introduction to Algorithms, Third Edition", \emph{The MIT Press}, 2009.

\end{thebibliography}
\fi

\newpage
\vspace{-1cm}
\input{hw1_pums_codebook}

\end{document}

